import os
import csv
import uuid
import glob
import shutil
import random

from pandas import DataFrame

import messages_pb2
import library

import batches
import regularizers
import scores
import scores_info as si


class ArtmModel(object):
    """ArtmModel represents a topic model (public class)

    Args:
      num_processors (int): how many threads will be used for model training,
      if not specified then number of threads will be detected by the library
      topic_names (list of str): names of topics in model, if not specified will be
      auto-generated by library according to num_topics
      num_topics (int): number of topics in model (is used if topic_names
      not specified), default=10
      class_ids (dict): list of class_ids and their weights to be used in model,
      key --- class_id, value --- weight, if not specified then all class_ids
      will be used
      num_document_passes (int): number of iterations over each document
      during processing, default=10
      cache_theta (bool): save or not the Theta matrix in model. Necessary
      if ArtmModel.get_theta() usage expects, default=True

    Important public fields:
      regularizers: contains dict of regularizers, included into model
      scores: contains dict of scores, included into model
      scores_info: contains dict of scoring results;
      key --- score name, value --- ScoreInfo object, which contains info about
      values of score on each synchronization in list

    NOTE:
      - Here and anywhere in BigARTM empty topic_names or class_ids means that
      model (or regularizer, or score) should use all topics or class_ids.
      - If some fields of regularizers or scores are not defined by
      user --- internal library defaults would be used.
      - If field 'topics_name' == [], it will be generated by BigARTM and will
      be available using ArtmModel.topics_name().
    """

    # ========== CONSTRUCTOR ==========
    def __init__(self, num_processors=0, topic_names=None, num_topics=10,
                 class_ids=None, num_document_passes=10, cache_theta=True):
        self._num_processors = 0
        self._num_topics = 10
        self._num_document_passes = 10
        self._cache_theta = True

        if topic_names is None or topic_names is []:
            self._topic_names = []
            if num_topics > 0:
                self._num_topics = num_topics
        else:
            self._topic_names = topic_names
            self._num_topics = len(topic_names)

        if class_ids is None:
            self._class_ids = {}
        elif len(class_ids) > 0:
            self._class_ids = class_ids

        if num_processors > 0:
            self._num_processors = num_processors

        if num_document_passes > 0:
            self._num_document_passes = num_document_passes

        if isinstance(cache_theta, bool):
            self._cache_theta = cache_theta

        self._master = library.MasterComponent()
        self._master.config().processors_count = self._num_processors
        self._master.config().cache_theta = cache_theta
        self._master.Reconfigure()

        self._model = 'pwt'
        self._regularizers = regularizers.Regularizers(self._master)
        self._scores = scores.Scores(self._master, self._model)

        self._scores_info = {}
        self._synchronizations_processed = 0
        self._initialized = False

    # ========== PROPERTIES ==========
    @property
    def num_processors(self):
        return self._num_processors

    @property
    def num_document_passes(self):
        return self._num_document_passes

    @property
    def cache_theta(self):
        return self._cache_theta

    @property
    def num_topics(self):
        return self._num_topics

    @property
    def topic_names(self):
        return self._topic_names

    @property
    def class_ids(self):
        return self._class_ids

    @property
    def regularizers(self):
        return self._regularizers

    @property
    def scores(self):
        return self._scores

    @property
    def scores_info(self):
        return self._scores_info

    @property
    def master(self):
        return self._master

    @property
    def num_phi_updates(self):
        return self._synchronizations_processed

    # ========== SETTERS ==========
    @num_processors.setter
    def num_processors(self, num_processors):
        if num_processors <= 0 or not isinstance(num_processors, int):
            raise IOError('Number of processors should be a positive integer')
        else:
            self._num_processors = num_processors
            self._master.config().processors_count = num_processors
            self._master.Reconfigure()

    @num_document_passes.setter
    def num_document_passes(self, num_document_passes):
        if num_document_passes <= 0 or not isinstance(num_document_passes, int):
            raise IOError("Number of passes through documents" +
                          "should be a positive integer")
        else:
            self._num_document_passes = num_document_passes

    @cache_theta.setter
    def cache_theta(self, cache_theta):
        if not isinstance(cache_theta, bool):
            raise IOError('cache_theta should be bool')
        else:
            self._cache_theta = cache_theta
            self._master.config().cache_theta = cache_theta
            self._master.Reconfigure()

    @num_topics.setter
    def num_topics(self, num_topics):
        if num_topics <= 0 or not isinstance(num_topics, int):
            raise IOError('Number of topics should be a positive integer')
        else:
            self._num_topics = num_topics

    @topic_names.setter
    def topic_names(self, topic_names):
        if len(topic_names) < 0:
            raise IOError('Number of topic names should be non-negative')
        else:
            self._topic_names = topic_names
            self._num_topics = len(topic_names)

    @class_ids.setter
    def class_ids(self, class_ids):
        if len(class_ids) < 0:
            raise IOError('Number of (class_id, class_weight) pairs should be non-negative')
        else:
            self._class_ids = class_ids

    # ========== METHODS ==========
    def load_dictionary(self, dictionary_name=None, dictionary_path=None):
        """ArtmModel.load_dictionary() --- load the BigARTM dictionary of
        the collection into the library

        Args:
          dictionary_name (str): the name of the dictionary in the library, default=None
          dictionary_path (str): full file name of the dictionary, default=None
        """
        if dictionary_path is not None and dictionary_name is not None:
            self._master.ImportDictionary(dictionary_name, dictionary_path)
        elif dictionary_path is None:
            raise IOError('ArtmModel.load_dictionary(): dictionary_path is None')
        else:
            raise IOError('ArtmModel.load_dictionary(): dictionary_name is None')

    def remove_dictionary(self, dictionary_name=None):
        """ArtmModel.remove_dictionary() --- remove the loaded BigARTM dictionary
        from the library

        Args:
          dictionary_name (str): the name of the dictionary in th library, default=None
        """
        if dictionary_name is not None:
            self._master.lib_.ArtmDisposeDictionary(self._master.id_, dictionary_name)
        else:
            raise IOError('ArtmModel.remove_dictionary(): dictionary_name is None')

    def fit_offline(self, collection_name=None, batches=None, data_path='',
                    num_collection_passes=1, decay_weight=0.0, apply_weight=1.0,
                    reset_theta_scores=False, data_format='batches', batch_size=1000):
        """ArtmModel.fit_offline() --- proceed the learning of
        topic model in off-line mode

        Args:
          collection_name (str): the name of text collection (required if
          data_format == 'bow_uci'), default=None
          batches (list of str): list of file names of batches to be processed.
          If not None, than data_format should be 'batches'. Format --- '*.batch',
          default=None
          data_path (str):
          1) if data_format == 'batches' => folder containing batches and dictionary;
          2) if data_format == 'bow_uci' => folder containing
            docword.collection_name.txt and vocab.collection_name.txt files;
          3) if data_format == 'vowpal_wabbit' => file in Vowpal Wabbit format;
          4) if data_format == 'plain_text' => file with text;
          default=''
          num_collection_passes (int): number of iterations over whole given
          collection, default=1
          decay_weight (int): coefficient for applying old n_wt counters,
          default=0.0 (apply_weight + decay_weight = 1.0)
          apply_weight (int): coefficient for applying new n_wt counters,
          default=1.0 (apply_weight + decay_weight = 1.0)
          reset_theta_scores (bool): reset accumulated Theta scores
          before learning, default=False
          data_format (str): the type of input data;
          1) 'batches' --- the data in format of BigARTM;
          2) 'bow_uci' --- Bag-Of-Words in UCI format;
          3) 'vowpal_wabbit' --- Vowpal Wabbit format;
          4) 'plain_text' --- source text;
          default='batches'

          Next argument has sense only if data_format is not 'batches'
          (e.g. parsing is necessary).
            batch_size (int): number of documents to be stored ineach batch,
            default=1000

        Note:
          ArtmModel.initialize() should be proceed before first call
          ArtmModel.fit_offline(), or it will be initialized by dictionary
          during first call.
        """
        if collection_name is None and data_format == 'bow_uci':
            raise IOError('ArtmModel.fit_offline(): No collection name was given')

        if not data_format == 'batches' and batches is not None:
            raise IOError("ArtmModel.fit_offline(): batches != None" +
                          "require data_format == batches")

        target_folder = data_path
        batches_list = []
        if data_format == 'batches':
            if batches is None:
                batches_list = glob.glob(data_path + '/*.batch')
                if len(batches_list) < 1:
                    raise RuntimeError('ArtmModel.fit_offline(): No batches were found')
            else:
                batches_list = [data_path + '/' + batch for batch in batches]

        elif data_format == 'bow_uci' or data_format == 'vowpal_wabbit':
            target_folder = data_path + '/batches_temp_' + str(random.uniform(0, 1))
            collection_parser_config = batches._create_parser_config(
                data_path,
                collection_name,
                target_folder,
                batch_size,
                data_format)

            library.Library().ParseCollection(collection_parser_config)
            batches_list = glob.glob(target_folder + '/*.batch')

        elif data_format == 'plain_text':
            raise NotImplementedError()
        else:
            raise IOError('ArtmModel.fit_offline(): Unknown data format')

        if not self._initialized:
            dictionary_name = 'dictionary' + str(uuid.uuid4())
            self._master.ImportDictionary(dictionary_name,
                                          os.path.join(target_folder, 'dictionary'))
            self.initialize(dictionary_name)
            self.remove_dictionary(dictionary_name)

        theta_regularizers, phi_regularizers = {}, {}
        for name, config in self._regularizers.data.iteritems():
            if str(config.__class__.__bases__[0].__name__) == 'BaseRegularizerTheta':
                theta_regularizers[name] = config.tau
            else:
                phi_regularizers[name] = config.tau

        for _ in xrange(num_collection_passes):
            self._master.ProcessBatches(pwt=self._model,
                                        batches=batches_list,
                                        target_nwt='nwt_hat',
                                        regularizers=theta_regularizers,
                                        inner_iterations_count=self._num_document_passes,
                                        class_ids=self._class_ids,
                                        reset_scores=reset_theta_scores)
            self._synchronizations_processed += 1
            if self._synchronizations_processed == 1:
                self._master.MergeModel({self._model: decay_weight, 'nwt_hat': apply_weight},
                                        target_nwt='nwt', topic_names=self._topic_names)
            else:
                self._master.MergeModel({'nwt': decay_weight, 'nwt_hat': apply_weight},
                                        target_nwt='nwt', topic_names=self._topic_names)

            self._master.RegularizeModel(self._model, 'nwt', 'rwt', phi_regularizers)
            self._master.NormalizeModel('nwt', self._model, 'rwt')

            for name in self.scores.data.keys():
                if name not in self.scores_info:
                    if self.scores[name].type == library.ScoreConfig_Type_SparsityPhi:
                        self._scores_info[name] = si.SparsityPhiScoreInfo(self.scores[name])
                    elif self.scores[name].type == library.ScoreConfig_Type_SparsityTheta:
                        self._scores_info[name] = si.SparsityThetaScoreInfo(self.scores[name])
                    elif self.scores[name].type == library.ScoreConfig_Type_Perplexity:
                        self._scores_info[name] = si.PerplexityScoreInfo(self.scores[name])
                    elif self.scores[name].type == library.ScoreConfig_Type_ThetaSnippet:
                        self._scores_info[name] = si.ThetaSnippetScoreInfo(self.scores[name])
                    elif self.scores[name].type == library.ScoreConfig_Type_ItemsProcessed:
                        self._scores_info[name] = si.ItemsProcessedScoreInfo(self.scores[name])
                    elif self.scores[name].type == library.ScoreConfig_Type_TopTokens:
                        self._scores_info[name] = si.TopTokensScoreInfo(self.scores[name])
                    elif self.scores[name].type == library.ScoreConfig_Type_TopicKernel:
                        self._scores_info[name] = si.TopicKernelScoreInfo(self.scores[name])

                    for _ in xrange(self._synchronizations_processed - 1):
                        self._scores_info[name].add()

                self._scores_info[name].add(self.scores[name])

        # Remove temp batches folder if it necessary
        if not data_format == 'batches':
            shutil.rmtree(target_folder)

    def fit_online(self, collection_name=None, batches=None, data_path='',
                   tau0=1024.0, kappa=0.7, update_every=1, reset_theta_scores=False,
                   data_format='batches', batch_size=1000):
        """ArtmModel.fit_online() --- proceed the learning of topic model
        in on-line mode

        Args:
          collection_name (str): the name of text collection (required if
          data_format == 'bow_uci'), default=None
          batches (list of str): list of file names of batches to be processed.
          If not None, than data_format should be 'batches'. Format --- '*.batch',
          default=None
          data_path (str):
          1) if data_format == 'batches' => folder containing batches and dictionary;
          2) if data_format == 'bow_uci' => folder containing
            docword.collection_name.txt and vocab.collection_name.txt files;
          3) if data_format == 'vowpal_wabbit' => file in Vowpal Wabbit format;
          4) if data_format == 'plain_text' => file with text;
          default=''
          update_every (int): the number of batches; model will be updated once per it,
          default=1
          tau0 (float): coefficient (see kappa), default=1024.0
          kappa (float): power for tau0, default=0.7

          The formulas for decay_weight and apply_weight:
          update_count = current_processed_docs / (batch_size * update_every)
          rho = pow(tau0 + update_count, -kappa)
          decay_weight = 1-rho
          apply_weight = rho

          reset_theta_scores (bool): reset accumulated Theta scores
          before learning, default=False
          data_format (str): the type of input data;
          1) 'batches' --- the data in format of BigARTM;
          2) 'bow_uci' --- Bag-Of-Words in UCI format;
          3) 'vowpal_wabbit' --- Vowpal Wabbit format;
          4) 'plain_text' --- source text;
          default='batches'

          Next argument has sense only if data_format is not 'batches'
          (e.g. parsing is necessary).
            batch_size (int): number of documents to be stored ineach batch,
            default=1000

        Note:
          ArtmModel.initialize() should be proceed before first call
          ArtmModel.fit_online(), or it will be initialized by dictionary
          during first call.
        """
        if collection_name is None and data_format == 'bow_uci':
            raise IOError('ArtmModel.fit_online(): No collection name was given')

        if not data_format == 'batches' and batches is not None:
            raise IOError('batches != None require data_format == batches')

        target_folder = data_path
        batches_list = []
        if data_format == 'batches':
            if batches is None:
                batches_list = glob.glob(data_path + '/*.batch')
                if len(batches_list) < 1:
                    raise RuntimeError('ArtmModel.fit_online(): No batches were found')
            else:
                batches_list = [data_path + '/' + batch for batch in batches]

        elif data_format == 'bow_uci' or data_format == 'vowpal_wabbit':
            target_folder = data_path + '/batches_temp_' + str(random.uniform(0, 1))
            collection_parser_config = batches._create_parser_config(
                data_path,
                collection_name,
                target_folder,
                batch_size,
                data_format)

            library.Library().ParseCollection(collection_parser_config)
            batches = glob.glob(target_folder + '/*.batch')

        elif data_format == 'plain_text':
            raise NotImplementedError()
        else:
            raise IOError('ArtmModel.fit_online(): Unknown data format')

        if not self._initialized:
            dictionary_name = 'dictionary' + str(uuid.uuid4())
            self._master.ImportDictionary(dictionary_name,
                                          os.path.join(target_folder, 'dictionary'))
            self.initialize(dictionary_name)
            self.remove_dictionary(dictionary_name)

        theta_regularizers, phi_regularizers = {}, {}
        for name, config in self._regularizers.data.iteritems():
            if str(config.__class__.__bases__[0]) == 'BaseRegularizerTheta':
                theta_regularizers[name] = config.tau
            else:
                phi_regularizers[name] = config.tau

        batches_to_process = []
        current_processed_documents = 0
        for batch_index, batch_filename in enumerate(batches_list):
            batches_to_process.append(batch_filename)
            if ((batch_index + 1) % update_every == 0) or ((batch_index + 1) == len(batches_list)):
                self._master.ProcessBatches(pwt=self._model,
                                            batches=batches_to_process,
                                            target_nwt='nwt_hat',
                                            regularizers=theta_regularizers,
                                            inner_iterations_count=self._num_document_passes,
                                            class_ids=self._class_ids,
                                            reset_scores=reset_theta_scores)

                current_processed_documents += batch_size * update_every
                update_count = current_processed_documents / (batch_size * update_every)
                rho = pow(tau0 + update_count, -kappa)
                decay_weight, apply_weight = 1 - rho, rho

                self._synchronizations_processed += 1
                if self._synchronizations_processed == 1:
                    self._master.MergeModel({self._model: decay_weight, 'nwt_hat': apply_weight},
                                            target_nwt='nwt', topic_names=self._topic_names)
                else:
                    self._master.MergeModel({'nwt': decay_weight, 'nwt_hat': apply_weight},
                                            target_nwt='nwt', topic_names=self._topic_names)

                self._master.RegularizeModel(self._model, 'nwt', 'rwt', phi_regularizers)
                self._master.NormalizeModel('nwt', self._model, 'rwt')
                batches_to_process = []

                for name in self.scores.data.keys():
                    if name not in self.scores_info:
                        if self.scores[name].type == library.ScoreConfig_Type_SparsityPhi:
                            self._scores_info[name] = si.SparsityPhiScoreInfo(self.scores[name])
                        elif self.scores[name].type == library.ScoreConfig_Type_SparsityTheta:
                            self._scores_info[name] = si.SparsityThetaScoreInfo(self.scores[name])
                        elif self.scores[name].type == library.ScoreConfig_Type_Perplexity:
                            self._scores_info[name] = si.PerplexityScoreInfo(self.scores[name])
                        elif self.scores[name].type == library.ScoreConfig_Type_ThetaSnippet:
                            self._scores_info[name] = si.ThetaSnippetScoreInfo(self.scores[name])
                        elif self.scores[name].type == library.ScoreConfig_Type_ItemsProcessed:
                            self._scores_info[name] = si.ItemsProcessedScoreInfo(self.scores[name])
                        elif self.scores[name].type == library.ScoreConfig_Type_TopTokens:
                            self._scores_info[name] = si.TopTokensScoreInfo(self.scores[name])
                        elif self.scores[name].type == library.ScoreConfig_Type_TopicKernel:
                            self._scores_info[name] = si.TopicKernelScoreInfo(self.scores[name])

                        for _ in xrange(self._synchronizations_processed - 1):
                            self._scores_info[name].add()

                    self._scores_info[name].add(self.scores[name])

        # Remove temp batches folder if it necessary
        if not data_format == 'batches':
            shutil.rmtree(target_folder)

    def save(self, file_name='artm_model'):
        """ArtmModel.save() --- save the topic model to disk

        Args:
          file_name (str): the name of file to store model, default='artm_model'
        """
        if not self._initialized:
            raise RuntimeError("Model does not exist yet. Use " +
                               "ArtmModel.initialize()/ArtmModel.fit_*()")

        if os.path.isfile(file_name):
            os.remove(file_name)
        self._master.ExportModel(self._model, file_name)

    def load(self, file_name):
        """ArtmModel.load() --- load the topic model,
        saved by ArtmModel.save(), from disk

        Args:
          file_name (str) --- the name of file containing model, no default

        Note:
          Loaded model will overwrite ArtmModel.topic_names and
          ArtmModel.num_topics fields. Also it will empty
          ArtmModel.scores_info.
        """
        self._master.ImportModel(self._model, file_name)
        self._initialized = True
        topic_model = self._master.GetTopicModel(model=self._model, use_matrix=False)
        self._topic_names = [topic_name for topic_name in topic_model.topic_name]
        self._num_topics = topic_model.topics_count

        # Remove all info about previous iterations
        self._scores_info = {}
        self._synchronizations_processed = 0

    def to_csv(self, file_name='artm_model.csv'):
        """ArtmModel.to_csv() --- save the topic model to disk in
        .csv format (can't be loaded back)

        Args:
          file_name (str): the name of file to store model, default='artm_model.csv'
        """
        if not self._initialized:
            raise RuntimeError("Model does not exist yet. Use " +
                               "ArtmModel.initialize()/ArtmModel.fit_*()")

        if os.path.isfile(file_name):
            os.remove(file_name)

        with open(file_name, 'wb') as csvfile:
            writer = csv.writer(csvfile, delimiter=';', quotechar=';', quoting=csv.QUOTE_MINIMAL)
            if len(self._topic_names) > 0:
                writer.writerow(['Token'] + ['Class ID'] +
                                ['TOPIC: ' + topic_name for topic_name in self._topic_names])

            model = self._master.GetTopicModel(model=self._model)
            index = -1
            for row in model[1]:
                index += 1
                writer.writerow(
                    [model[0].token[index]] + [model[0].class_id[index]] +
                    [round(token_w, 5) for token_w in row])

    def get_phi(self, topic_names=None, class_ids=None):
        """ArtmModel.get_phi() --- get Phi matrix of model

        Args:
          topic_names (list of str): list with topics to extract,
          default=None (means all topics)
          class_ids (list of str): list with class ids to extract,
          default=None (means all class ids)

        Returns:
          pandas.DataFrame: (data, columns, rows), where:
          1) columns --- the names of topics in topic model
          2) rows --- the tokens of topic model
          3) data --- content of Phi matrix
        """
        if not self._initialized:
            raise RuntimeError("Model does not exist yet. Use " +
                               "ArtmModel.initialize()/ArtmModel.fit_*()")

        topic_model = self._master.GetTopicModel(model=self._model,
                                                 class_ids=class_ids,
                                                 topic_names=topic_names)
        tokens = [token for token in topic_model[0].token]
        topic_names = [topic_name for topic_name in topic_model[0].topic_name]
        retval = DataFrame(data=topic_model[1],
                           columns=topic_names,
                           index=tokens)

        return retval

    def get_theta(self, remove_theta=False):
        """ArtmModel.get_theta() --- get Theta matrix for training set
        of documents

        Args:
          remove_theta (bool): flag indicates save or remove Theta from model
          after extraction, default=False

        Returns:
          pandas.DataFrame: (data, columns, rows), where:
          1) columns --- the ids of documents, for which the Theta
          matrix was requested
          2) rows --- the names of topics in topic model, that was
          used to create Theta
          3) data --- content of Theta matrix
        """
        if self.cache_theta is False:
            raise ValueError("ArtmModel.get_theta(): cache_theta == False" +
                             "Set ArtmModel.cache_theta = True")
        if not self._initialized:
            raise RuntimeError("ArtmModel.get_theta(): Model does not exist yet. Use " +
                               "ArtmModel.initialize()/ArtmModel.fit_*()")

        theta_matrix = self._master.GetThetaMatrix(self._model, clean_cache=remove_theta)
        document_ids = [item_id for item_id in theta_matrix[0].item_id]
        topic_names = [topic_name for topic_name in theta_matrix[0].topic_name]
        retval = DataFrame(data=theta_matrix[1].transpose(),
                           columns=document_ids,
                           index=topic_names)

        return retval

    def find_theta(self, batches=None, collection_name=None,
                   data_path='', data_format='batches'):
        """ArtmModel.find_theta() --- find Theta matrix for new documents

        Args:
          collection_name (str): the name of text collection (required if
          data_format == 'bow_uci'), default=None
          batches (list of str): list of file names of batches to be processed;
          if not None, than data_format should be 'batches'; format '*.batch',
          default=None
          data_path (str):
          1) if data_format == 'batches' =>
          folder containing batches and dictionary;
          2) if data_format == 'bow_uci' =>
          folder containing docword.txt and vocab.txt files;
          3) if data_format == 'vowpal_wabbit' => file in Vowpal Wabbit format;
          4) if data_format == 'plain_text' => file with text;
          default=''
          data_format (str): the type of input data;
          1) 'batches' --- the data in format of BigARTM;
          2) 'bow_uci' --- Bag-Of-Words in UCI format;
          3) 'vowpal_wabbit' --- Vowpal Wabbit format;
          4) 'plain_text' --- source text;
          default='batches'

        Returns:
          pandas.DataFrame: (data, columns, rows), where:
          1) columns --- the ids of documents, for which the Theta
          matrix was requested
          2) rows --- the names of topics in topic model, that was
          used to create Theta
          3) data --- content of Theta matrix.
        """
        if collection_name is None and data_format == 'bow_uci':
            raise IOError('ArtmModel.find_theta(): No collection name was given')

        if not data_format == 'batches' and batches is not None:
            raise IOError("ArtmModel.find_theta(): batches != None require" +
                          "data_format == batches")

        if not self._initialized:
            raise RuntimeError("ArtmModel.find_theta(): Model does not exist yet. Use " +
                               "ArtmModel.initialize()/ArtmModel.fit_*()")

        target_folder = data_path + '/batches_temp_' + str(random.uniform(0, 1))
        batches_list = []
        if data_format == 'batches':
            if batches is None:
                batches_list = glob.glob(data_path + '/*.batch')
                if len(batches_list) < 1:
                    raise RuntimeError('ArtmModel.find_theta(): No batches were found')
            else:
                batches_list = [data_path + '/' + batch for batch in batches]

        elif data_format == 'bow_uci' or data_format == 'vowpal_wabbit':
            collection_parser_config = batches._create_parser_config(
                data_path=data_path,
                collection_name=collection_name,
                target_folder=target_folder,
                batch_size=1000,
                data_format=data_format)

            library.Library().ParseCollection(collection_parser_config)
            batches_list = glob.glob(target_folder + '/*.batch')

        elif data_format == 'plain_text':
            raise NotImplementedError()
        else:
            raise IOError('ArtmModel.find_theta(): Unknown data format')

        results = self._master.ProcessBatches(
            pwt=self._model,
            batches=batches_list,
            target_nwt='nwt_hat',
            inner_iterations_count=self._num_document_passes,
            class_ids=self._class_ids,
            theta_matrix_type=library.ProcessBatchesArgs_ThetaMatrixType_External)

        document_ids = [item_id for item_id in results[0].theta_matrix.item_id]
        topic_names = [topic_name for topic_name in results[0].theta_matrix.topic_name]
        retval = DataFrame(data=results[1].transpose(),
                           columns=document_ids,
                           index=topic_names)

        # Remove temp batches folder if necessary
        if not data_format == 'batches':
            shutil.rmtree(target_folder)

        return retval

    def initialize(self, data_path=None, dictionary_name=None):
        """ArtmModel.initialize() --- initialize topic model before learning

        Args:
          data_path (str): name of directory containing BigARTM batches, default=None
          dictionary_name (str): the name of loaded BigARTM collection
          dictionary, default=None

        Note:
          Priority of initialization:
          1) batches in 'data_path'
          2) dictionary
        """
        if data_path is not None:
            self._master.InitializeModel(model_name=self._model,
                                         batch_folder=data_path,
                                         topics_count=self._num_topics,
                                         topic_names=self._topic_names)
        else:
            self._master.InitializeModel(model_name=self._model,
                                         dictionary_name=dictionary_name,
                                         topics_count=self._num_topics,
                                         topic_names=self._topic_names)

        topic_model = self._master.GetTopicModel(model=self._model, use_matrix=False)
        self._topic_names = [topic_name for topic_name in topic_model.topic_name]
        self._initialized = True

        # Remove all info about previous iterations
        self._scores_info = {}
        self._synchronizations_processed = 0
